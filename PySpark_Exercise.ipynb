{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<i>Copyright (c) Microsoft Corporation. All rights reserved.</i>\n",
        "\n",
        "<i>Licensed under the MIT License.</i>"
      ],
      "metadata": {
        "id": "Nr_18Dp_rqRm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running ALS on MovieLens (PySpark)\n",
        "\n",
        "Matrix factorization by [ALS](https://spark.apache.org/docs/latest/api/python/_modules/pyspark/ml/recommendation.html#ALS) (Alternating Least Squares) is a well known collaborative filtering algorithm.\n",
        "\n",
        "This notebook provides an example of how to utilize and evaluate ALS PySpark ML (DataFrame-based API) implementation, meant for large-scale distributed datasets. We use a smaller dataset in this example to run ALS efficiently on multiple cores of a [Data Science Virtual Machine](https://azure.microsoft.com/en-gb/services/virtual-machines/data-science-virtual-machines/)."
      ],
      "metadata": {
        "id": "lk3m7e_vrqRp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: This notebook requires a PySpark environment to run properly. Please follow the steps in [SETUP.md](../../SETUP.md) to install the PySpark environment."
      ],
      "metadata": {
        "id": "OndLpX8FrqRp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "source": [
        "# set the environment path to find Recommenders\n",
        "! pip install pyspark\n",
        "import sys\n",
        "import pyspark\n",
        "from pyspark.ml.recommendation import ALS\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.types import StructType, StructField\n",
        "from pyspark.sql.types import StringType, FloatType, IntegerType, LongType\n",
        "\n",
        "! pip install git+https://github.com/microsoft/recommenders.git\n",
        "from recommenders.utils.timer import Timer\n",
        "from recommenders.datasets import movielens\n",
        "from recommenders.utils.notebook_utils import is_jupyter\n",
        "from recommenders.datasets.spark_splitters import spark_random_split\n",
        "from recommenders.evaluation.spark_evaluation import SparkRatingEvaluation, SparkRankingEvaluation\n",
        "from recommenders.utils.spark_utils import start_or_get_spark\n",
        "\n",
        "print(\"System version: {}\".format(sys.version))\n",
        "print(\"Spark version: {}\".format(pyspark.__version__))\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.7/dist-packages (3.2.0)\n",
            "Requirement already satisfied: py4j==0.10.9.2 in /usr/local/lib/python3.7/dist-packages (from pyspark) (0.10.9.2)\n",
            "Collecting git+https://github.com/microsoft/recommenders.git\n",
            "  Cloning https://github.com/microsoft/recommenders.git to /tmp/pip-req-build-_4mvndor\n",
            "  Running command git clone -q https://github.com/microsoft/recommenders.git /tmp/pip-req-build-_4mvndor\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pymanopt@ https://github.com/pymanopt/pymanopt/archive/fb36a272cdeecb21992cfd9271eb82baafeb316d.zip\n",
            "  Using cached https://github.com/pymanopt/pymanopt/archive/fb36a272cdeecb21992cfd9271eb82baafeb316d.zip\n",
            "Requirement already satisfied: retrying>=1.3.3 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (1.3.3)\n",
            "Requirement already satisfied: scipy<2,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (1.4.1)\n",
            "Requirement already satisfied: pyyaml<6,>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (5.4.1)\n",
            "Requirement already satisfied: pydocumentdb>=2.3.3<3 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (2.3.5)\n",
            "Requirement already satisfied: scikit-learn<1,>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (0.24.2)\n",
            "Requirement already satisfied: transformers<5,>=2.5.0 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (4.14.1)\n",
            "Requirement already satisfied: bottleneck<2,>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (1.3.2)\n",
            "Requirement already satisfied: pandera[strategies]>=0.6.5 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (0.8.0)\n",
            "Requirement already satisfied: numba<1,>=0.38.1 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (0.51.2)\n",
            "Requirement already satisfied: pandas<2,>1.0.3 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (1.1.5)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (2.23.0)\n",
            "Requirement already satisfied: category-encoders<2,>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (1.3.0)\n",
            "Requirement already satisfied: memory-profiler<1,>=0.54.0 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (0.60.0)\n",
            "Requirement already satisfied: seaborn<1,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (0.11.2)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (1.19.5)\n",
            "Requirement already satisfied: jinja2<4,>=2 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (2.11.3)\n",
            "Requirement already satisfied: cornac<2,>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (1.14.1)\n",
            "Requirement already satisfied: lightgbm>=2.2.1 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (2.2.3)\n",
            "Requirement already satisfied: tqdm<5,>=4.31.1 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (4.62.3)\n",
            "Requirement already satisfied: lightfm<2,>=1.15 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (1.16)\n",
            "Requirement already satisfied: nltk<4,>=3.4 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (3.6.6)\n",
            "Requirement already satisfied: matplotlib<4,>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from recommenders==1.0.0) (3.2.2)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders==1.0.0) (0.5.2)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.7/dist-packages (from category-encoders<2,>=1.3.0->recommenders==1.0.0) (0.10.2)\n",
            "Requirement already satisfied: powerlaw in /usr/local/lib/python3.7/dist-packages (from cornac<2,>=1.1.2->recommenders==1.0.0) (1.5)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2<4,>=2->recommenders==1.0.0) (2.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders==1.0.0) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders==1.0.0) (3.0.6)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders==1.0.0) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib<4,>=2.2.2->recommenders==1.0.0) (0.11.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from memory-profiler<1,>=0.54.0->recommenders==1.0.0) (5.4.8)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders==1.0.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders==1.0.0) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk<4,>=3.4->recommenders==1.0.0) (2021.11.10)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders==1.0.0) (0.34.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba<1,>=0.38.1->recommenders==1.0.0) (57.4.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas<2,>1.0.3->recommenders==1.0.0) (2018.9)\n",
            "Requirement already satisfied: typing-inspect>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders==1.0.0) (0.7.1)\n",
            "Requirement already satisfied: pandas-stubs in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders==1.0.0) (1.2.0.39)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders==1.0.0) (1.13.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders==1.0.0) (3.10.0.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders==1.0.0) (3.0.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders==1.0.0) (21.3)\n",
            "Requirement already satisfied: hypothesis>=5.41.1 in /usr/local/lib/python3.7/dist-packages (from pandera[strategies]>=0.6.5->recommenders==1.0.0) (6.31.6)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders==1.0.0) (2.4.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from hypothesis>=5.41.1->pandera[strategies]>=0.6.5->recommenders==1.0.0) (21.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->category-encoders<2,>=1.3.0->recommenders==1.0.0) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders==1.0.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders==1.0.0) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders==1.0.0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->recommenders==1.0.0) (3.0.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<1,>=0.22.1->recommenders==1.0.0) (3.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders==1.0.0) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders==1.0.0) (0.2.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders==1.0.0) (4.8.2)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders==1.0.0) (0.0.46)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=2.5.0->recommenders==1.0.0) (0.10.3)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from typing-inspect>=0.6.0->pandera[strategies]>=0.6.5->recommenders==1.0.0) (0.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=2.5.0->recommenders==1.0.0) (3.6.0)\n",
            "Requirement already satisfied: mpmath in /usr/local/lib/python3.7/dist-packages (from powerlaw->cornac<2,>=1.1.2->recommenders==1.0.0) (1.2.1)\n",
            "System version: 3.7.12 (default, Sep 10 2021, 00:21:48) \n",
            "[GCC 7.5.0]\n",
            "Spark version: 3.2.0\n"
          ]
        }
      ],
      "metadata": {
        "id": "VUp4tU-KrqRq",
        "outputId": "5241b296-7cab-4924-fd60-9dd9b22e7db9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the default parameters."
      ],
      "metadata": {
        "id": "ZRZk0SNHrqRt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "source": [
        "# top k items to recommend\n",
        "TOP_K = 10\n",
        "\n",
        "# Select MovieLens data size: 100k, 1m, 10m, or 20m\n",
        "MOVIELENS_DATA_SIZE = '100k'\n",
        "\n",
        "# Column names for the dataset\n",
        "COL_USER = \"UserId\"\n",
        "COL_ITEM = \"MovieId\"\n",
        "COL_RATING = \"Rating\"\n",
        "COL_TIMESTAMP = \"Timestamp\""
      ],
      "outputs": [],
      "metadata": {
        "tags": [
          "parameters"
        ],
        "id": "8SaGCFUZrqRu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Set up Spark context\n",
        "\n",
        "The following settings work well for debugging locally on VM - change when running on a cluster. We set up a giant single executor with many threads and specify memory cap. "
      ],
      "metadata": {
        "id": "WG-N4fDJrqRu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "source": [
        "# the following settings work well for debugging locally on VM - change when running on a cluster\n",
        "# set up a giant single executor with many threads and specify memory cap\n",
        "spark = start_or_get_spark(\"ALS PySpark\", memory=\"16g\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "R2kJuLLkrqRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Download the MovieLens dataset"
      ],
      "metadata": {
        "id": "RK7dFK1prqRv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "source": [
        "# Note: The DataFrame-based API for ALS currently only supports integers for user and item ids.\n",
        "schema = StructType(\n",
        "    (\n",
        "        StructField(COL_USER, IntegerType()),\n",
        "        StructField(COL_ITEM, IntegerType()),\n",
        "        StructField(COL_RATING, FloatType()),\n",
        "        StructField(COL_TIMESTAMP, LongType()),\n",
        "    )\n",
        ")\n",
        "\n",
        "data = movielens.load_spark_df(spark, size=MOVIELENS_DATA_SIZE, schema=schema)\n",
        "data.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.81k/4.81k [00:00<00:00, 9.75kKB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+------+---------+\n",
            "|UserId|MovieId|Rating|Timestamp|\n",
            "+------+-------+------+---------+\n",
            "|   196|    242|   3.0|881250949|\n",
            "|   186|    302|   3.0|891717742|\n",
            "|    22|    377|   1.0|878887116|\n",
            "|   244|     51|   2.0|880606923|\n",
            "|   166|    346|   1.0|886397596|\n",
            "|   298|    474|   4.0|884182806|\n",
            "|   115|    265|   2.0|881171488|\n",
            "|   253|    465|   5.0|891628467|\n",
            "|   305|    451|   3.0|886324817|\n",
            "|     6|     86|   3.0|883603013|\n",
            "|    62|    257|   2.0|879372434|\n",
            "|   286|   1014|   5.0|879781125|\n",
            "|   200|    222|   5.0|876042340|\n",
            "|   210|     40|   3.0|891035994|\n",
            "|   224|     29|   3.0|888104457|\n",
            "|   303|    785|   3.0|879485318|\n",
            "|   122|    387|   5.0|879270459|\n",
            "|   194|    274|   2.0|879539794|\n",
            "|   291|   1042|   4.0|874834944|\n",
            "|   234|   1184|   2.0|892079237|\n",
            "+------+-------+------+---------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "tmPgPrNmrqRw",
        "outputId": "0788c12c-3d1b-45a8-b405-470dbbde4357",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Split the data using the Spark random splitter provided in utilities"
      ],
      "metadata": {
        "id": "XkwAsQARrqRx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "source": [
        "train, test = spark_random_split(data, ratio=0.75, seed=123)\n",
        "print (\"N train\", train.cache().count())\n",
        "print (\"N test\", test.cache().count())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "N train 75018\n",
            "N test 24982\n"
          ]
        }
      ],
      "metadata": {
        "id": "nngUEc5UrqRx",
        "outputId": "a7b18e29-3680-44e7-c0a5-d926b67cf5d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Train the ALS model on the training data, and get the top-k recommendations for our testing data\n",
        "\n",
        "To predict movie ratings, we use the rating data in the training set as users' explicit feedback. The hyperparameters used in building the model are referenced from [here](http://mymedialite.net/examples/datasets.html). We do not constrain the latent factors (`nonnegative = False`) in order to allow for both positive and negative preferences towards movies.\n",
        "Timing will vary depending on the machine being used to train."
      ],
      "metadata": {
        "id": "XSAliR6qrqRx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "source": [
        "header = {\n",
        "    \"userCol\": COL_USER,\n",
        "    \"itemCol\": COL_ITEM,\n",
        "    \"ratingCol\": COL_RATING,\n",
        "}\n",
        "\n",
        "\n",
        "als = ALS(\n",
        "    rank=10,\n",
        "    maxIter=15,\n",
        "    implicitPrefs=False,\n",
        "    regParam=0.05,\n",
        "    coldStartStrategy='drop',\n",
        "    nonnegative=False,\n",
        "    seed=42,\n",
        "    **header\n",
        ")"
      ],
      "outputs": [],
      "metadata": {
        "id": "FWsnYXWfrqRy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "source": [
        "with Timer() as train_time:\n",
        "    model = als.fit(train)\n",
        "\n",
        "print(\"Took {} seconds for training.\".format(train_time.interval))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Took 12.691781000999981 seconds for training.\n"
          ]
        }
      ],
      "metadata": {
        "id": "dT5nH7kurqRy",
        "outputId": "b2c0e001-f8e2-41b3-c1a0-653ab8fc21df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the movie recommendation use case, recommending movies that have been rated by the users do not make sense. Therefore, the rated movies are removed from the recommended items.\n",
        "\n",
        "In order to achieve this, we recommend all movies to all users, and then remove the user-movie pairs that exist in the training dataset."
      ],
      "metadata": {
        "id": "pc2pE94BrqRy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "source": [
        "with Timer() as test_time:\n",
        "\n",
        "    # Get the cross join of all user-item pairs and score them.\n",
        "    users = train.select(COL_USER).distinct()\n",
        "    items = train.select(COL_ITEM).distinct()\n",
        "    user_item = users.crossJoin(items)\n",
        "    dfs_pred = model.transform(user_item)\n",
        "\n",
        "    # Remove seen items.\n",
        "    dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n",
        "        train.alias(\"train\"),\n",
        "        (dfs_pred[COL_USER] == train[COL_USER]) & (dfs_pred[COL_ITEM] == train[COL_ITEM]),\n",
        "        how='outer'\n",
        "    )\n",
        "\n",
        "    top_all = dfs_pred_exclude_train.filter(dfs_pred_exclude_train[f\"train.{COL_RATING}\"].isNull()) \\\n",
        "        .select('pred.' + COL_USER, 'pred.' + COL_ITEM, 'pred.' + \"prediction\")\n",
        "\n",
        "    # In Spark, transformations are lazy evaluation\n",
        "    # Use an action to force execute and measure the test time \n",
        "    top_all.cache().count()\n",
        "\n",
        "print(\"Took {} seconds for prediction.\".format(test_time.interval))"
      ],
      "outputs": [
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9789de4855e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     dfs_pred_exclude_train = dfs_pred.alias(\"pred\").join(\n\u001b[1;32m     11\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malias\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mdfs_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOL_USER\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOL_USER\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdfs_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOL_ITEM\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCOL_ITEM\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     )\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   1353\u001b[0m                 \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how should be a string\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1355\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1356\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1308\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1310\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m:  Column MovieId#1288 are ambiguous. It's probably because you joined several Datasets together, and some of these Datasets are the same. This column points to one of the Datasets but Spark is unable to figure out which one. Please alias the Datasets with different names via `Dataset.as` before joining them, and specify the column using qualified name, e.g. `df.as(\"a\").join(df.as(\"b\"), $\"a.id\" > $\"b.id\")`. You can also set spark.sql.analyzer.failAmbiguousSelfJoin to false to disable this check.        "
          ]
        }
      ],
      "metadata": {
        "id": "332MBanxrqRz",
        "outputId": "402b6444-1f69-406b-c150-69f71eaeedd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "top_all.show()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+----------+\n",
            "|UserId|MovieId|prediction|\n",
            "+------+-------+----------+\n",
            "|     1|    587| 3.0676804|\n",
            "|     1|    869| 2.4396753|\n",
            "|     1|   1208| 3.2788403|\n",
            "|     1|   1357| 2.0567489|\n",
            "|     1|   1677| 2.9661644|\n",
            "|     2|     80| 2.3442159|\n",
            "|     2|    472|  3.060428|\n",
            "|     2|    582|  3.489215|\n",
            "|     2|    838| 1.0985656|\n",
            "|     2|    975| 1.8764799|\n",
            "|     2|   1260| 3.0814102|\n",
            "|     2|   1381|  3.288192|\n",
            "|     2|   1530| 1.9368806|\n",
            "|     3|     22| 4.2560363|\n",
            "|     3|     57|  3.295701|\n",
            "|     3|     89|  4.983886|\n",
            "|     3|    367| 2.5427854|\n",
            "|     3|   1091| 1.4424214|\n",
            "|     3|   1167| 2.2066739|\n",
            "|     3|   1499|  3.368075|\n",
            "+------+-------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "BA0llHUnrqRz",
        "outputId": "cccf2d22-1352-425b-b0cb-42fde38eb5ed"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Evaluate how well ALS performs"
      ],
      "metadata": {
        "id": "BKKxlAa9rqRz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "rank_eval = SparkRankingEvaluation(test, top_all, k = TOP_K, col_user=COL_USER, col_item=COL_ITEM, \n",
        "                                    col_rating=COL_RATING, col_prediction=\"prediction\", \n",
        "                                    relevancy_method=\"top_k\")"
      ],
      "outputs": [],
      "metadata": {
        "id": "siUdKm_VrqR0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "print(\"Model:\\tALS\",\n",
        "      \"Top K:\\t%d\" % rank_eval.k,\n",
        "      \"MAP:\\t%f\" % rank_eval.map_at_k(),\n",
        "      \"NDCG:\\t%f\" % rank_eval.ndcg_at_k(),\n",
        "      \"Precision@K:\\t%f\" % rank_eval.precision_at_k(),\n",
        "      \"Recall@K:\\t%f\" % rank_eval.recall_at_k(), sep='\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:\tALS\n",
            "Top K:\t10\n",
            "MAP:\t0.005734\n",
            "NDCG:\t0.047460\n",
            "Precision@K:\t0.051911\n",
            "Recall@K:\t0.017514\n"
          ]
        }
      ],
      "metadata": {
        "id": "UPV00TTFrqR0",
        "outputId": "f63aec6b-30b7-417d-f536-749e17cfe3f9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Evaluate rating prediction"
      ],
      "metadata": {
        "id": "xWnqU3UnrqR0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# Generate predicted ratings.\n",
        "prediction = model.transform(test)\n",
        "prediction.cache().show()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-------+------+---------+----------+\n",
            "|UserId|MovieId|Rating|Timestamp|prediction|\n",
            "+------+-------+------+---------+----------+\n",
            "|   406|    148|   3.0|879540276| 2.2832825|\n",
            "|    27|    148|   3.0|891543129| 1.7940072|\n",
            "|   606|    148|   3.0|878150506| 3.7863157|\n",
            "|   916|    148|   2.0|880843892| 2.3045797|\n",
            "|   236|    148|   4.0|890117028| 1.9480721|\n",
            "|   602|    148|   4.0|888638517| 3.1172547|\n",
            "|   663|    148|   4.0|889492989| 2.7976327|\n",
            "|   372|    148|   5.0|876869915|  4.170663|\n",
            "|   190|    148|   4.0|891033742| 3.6491241|\n",
            "|     1|    148|   2.0|875240799|  2.829558|\n",
            "|   297|    148|   3.0|875239619| 2.1554093|\n",
            "|   178|    148|   4.0|882824325|  3.932391|\n",
            "|   308|    148|   3.0|887740788| 2.9132738|\n",
            "|   923|    148|   4.0|880387474| 3.5403519|\n",
            "|    54|    148|   3.0|880937490|  3.165133|\n",
            "|   430|    148|   2.0|877226047|  2.891675|\n",
            "|    92|    148|   2.0|877383934| 2.6483998|\n",
            "|   447|    148|   4.0|878854729| 3.1101565|\n",
            "|   374|    148|   4.0|880392992| 2.2130618|\n",
            "|   891|    148|   5.0|891639793|  3.138905|\n",
            "+------+-------+------+---------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "metadata": {
        "id": "KYJioYd2rqR0",
        "outputId": "11fbfe5e-90fb-4eac-9775-cb78ada5c784"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "rating_eval = SparkRatingEvaluation(test, prediction, col_user=COL_USER, col_item=COL_ITEM, \n",
        "                                    col_rating=COL_RATING, col_prediction=\"prediction\")\n",
        "\n",
        "print(\"Model:\\tALS rating prediction\",\n",
        "      \"RMSE:\\t%f\" % rating_eval.rmse(),\n",
        "      \"MAE:\\t%f\" % rating_eval.mae(),\n",
        "      \"Explained variance:\\t%f\" % rating_eval.exp_var(),\n",
        "      \"R squared:\\t%f\" % rating_eval.rsquared(), sep='\\n')"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model:\tALS rating prediction\n",
            "RMSE:\t0.967296\n",
            "MAE:\t0.753306\n",
            "Explained variance:\t0.261864\n",
            "R squared:\t0.255480\n"
          ]
        }
      ],
      "metadata": {
        "id": "w9i6KflQrqR1",
        "outputId": "ff941c67-e383-4875-cfba-8e0c92d866d5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "if is_jupyter():\n",
        "    # Record results with papermill for tests\n",
        "    import papermill as pm\n",
        "    import scrapbook as sb\n",
        "    sb.glue(\"map\", rank_eval.map_at_k())\n",
        "    sb.glue(\"ndcg\", rank_eval.ndcg_at_k())\n",
        "    sb.glue(\"precision\", rank_eval.precision_at_k())\n",
        "    sb.glue(\"recall\", rank_eval.recall_at_k())\n",
        "    sb.glue(\"rmse\", rating_eval.rmse())\n",
        "    sb.glue(\"mae\", rating_eval.mae())\n",
        "    sb.glue(\"exp_var\", rating_eval.exp_var())\n",
        "    sb.glue(\"rsquared\", rating_eval.rsquared())\n",
        "    sb.glue(\"train_time\", train_time.interval)\n",
        "    sb.glue(\"test_time\", test_time.interval)"
      ],
      "outputs": [],
      "metadata": {
        "id": "pPnPS2_trqR1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "source": [
        "# cleanup spark instance\n",
        "spark.stop()"
      ],
      "outputs": [],
      "metadata": {
        "id": "uFCdPuBirqR1"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (reco_pyspark)",
      "language": "python",
      "name": "reco_pyspark"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    },
    "colab": {
      "name": "als_movielens.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}